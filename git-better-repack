#! /usr/bin/python

# Copyright 2017 by Vincent Fourmond
# This program is free software. It is distributed under the terms of the
# GNU General Public License version 3.0 or later, found at
# https://www.gnu.org/copyleft/gpl.html
#
# There is NO WARRANTY.

# I'm unsure what this does as of now, but it's going to be fun !

import os
import re
import os.path
import glob

import argparse
import random
import shutil
import subprocess

# For categorizing files.
import magic

parser = argparse.ArgumentParser()
parser.add_argument("action", default=['dump-objects'], nargs='*',
                    help="action to be performed")
parser.add_argument("-s", "--size", type=int,
                    help="size (MB)")
parser.add_argument("--big-objects", type=int,
                    help="size (kb)")
parser.add_argument("--mess-up", type=int,
                    help="create a fake repository based on the files in the current directory with several operations")
parser.add_argument("--dump-pack", type=str,
                    help="dumps pack contents")
parser.add_argument("-n", "--dry-run",action="store_true",
                    help="dry run (don't repack)")
parser.add_argument("-r", "--repack-size", type=int,
                    help="size of loose objects necessary to run git-repack (MB)")
args = parser.parse_args()

# Provide default values
if args.size:                   # in bytes !
    args.size *= 1024**2
else:
    args.size = 10*1024**2

if args.big_objects:                   # in bytes !
    args.big_objects *= 1024
else:
    args.big_objects = 1*1024**2

if args.repack_size:            
    args.repack_size *= 1024 # in KB, compared to count_objects
else:
    args.repack_size = 10*1024



class git_rep:
    """A whole git repository"""

    def __init__(self):
        """Creates a repository object referring to the current directory"""
        if os.path.isdir(".git/objects"):
            self.obj_dir = ".git/objects"
        elif os.path.isdir("objects"):
            self.obj_dir = "objects" # bare repository
        else:
            raise RuntimeError("Must be at the root of a git repository")

    def list_packs(self):
        """Finds all pack files, and record them. Does not read them, though,
as it can be soo long !"""
        self.packs = []
        for pck in glob.glob(self.obj_dir + "/pack/*.pack"):
            self.packs.append(packfile(pck, self))
        return self.packs


    def count_objects(self):
        rv = dict()
        with os.popen('git count-objects -v') as gco:
            for line in gco:
                lst = line.split(":")
                rv[lst[0]] = int(lst[1])
        self.count_objects = rv
        return self.count_objects

    def cheap_gc(self):
        """Runs git repack to pack loose objects and cleanup packs. A weak but
cheap version of git gc"""
        os.system("git repack -d")

    def drop_redundant_packs(self):
        with os.popen('git pack-redundant --all') as gpr:
            for line in gpr:
                pck = line.rstrip()
                print "Removing redundant pack: %s" % pck
                os.unlink(pck)

    def simple_repack(self, size_threshold, tolerance = 1.5,
                      dry_run = False):
        """Given a threshold of pack size, consolidate all packs significantly
smaller than the threshold into single packs whose size shouldn't
exceed tolerance times the threshold."""
        self.cheap_gc()
        self.drop_redundant_packs()
        packs = []
        ts = 0
        rt = size_threshold * tolerance * 0.5
        for pack in rep.list_packs():
            print pack
            if pack.size < rt:
                pack.read_pack()
                packs.append(pack)
                ts += pack.size
                if ts > rt:
                    packfile.repack(packs, dry_run)
                    packs = []
                    ts = 0
                    print pack
            else:
                print "Pack %s too big, not touching it" % pack
        if len(packs) > 1:
            packfile.repack(packs, dry_run)
        self.drop_redundant_packs()

    def repack_v2(self, size_threshold, tolerance = 1.5, dry_run = False):
        """Attemps to be more clever than simple_repack by maximizing the chances of deltas"""
        self.cheap_gc()
        self.drop_redundant_packs()
        packs = []
        ts = 0
        rt = size_threshold * tolerance * 0.5
        for pack in rep.list_packs():
            if pack.size < rt:
                pack.read_pack()
        # Hmmm need to check the file types, but also try to avoid
        # generating too many packs...
        

    def cross_reference_objects(self):
        """Runs over all the packs, and records all the objects' SHA1 together
with the packs they are found in. Assumes the packs are already read. Returns the shas of duplicate objects"""
        self.all_objects = {}
        dups = []
        for pck in self.packs:
            for obj in pck.objects.values():
                if not obj.sha1 in self.all_objects:
                    self.all_objects[obj.sha1] = []
                else:
                    dups.append(obj.sha1)
                self.all_objects[obj.sha1].append(pck)
        return dups

    def dump_objects(self, full = False):
        st = {}
        ics = []
        for pck in self.list_packs():
            print "Reading pack %s" % pck.pack_file
            pck.read_pack()
            pck.read_object_sizes()
            sts = pck.stats()
            print " -> %s " % sts
            for n in sts.keys():
                st[n] = st.get(n, 0) + sts[n]
            if full:
                for ic in pck.interconnected_objects.values():
                    print "   - %s " % ic.stats()
                    ics.append(ic)
                    
        dups = self.cross_reference_objects()
        print "%d duplicate objects" % len(dups)
        print st

        if full:
            print "Hierarchies by size"
            ics = sorted(ics, key=lambda a: a.stats()['packed_size'])
            for ic in ics:
                print "   - %s " % ic.stats()

        # Now make stats about the hierarchies ?


    def smart_repack(self, small_obj_size, pack_size,
                     tolerance = 1.2, dry_run = True):
        """This function repacks the whole repository sticking to those rules:
          * packs should not be (much) larger than pack_size
          * objects -- or, rather hierarchies --  smaller than small_obj_size
            are packed together
          * and, consequently, objects larger than small_obj_size are
            packed together
          * existing hierarchies are preserved
          * packs adhering to the rules are left untouched
          small_obj_size and pack_size are in bytes
        """

        # if not dry_run:
        #     self.cheap_gc()
        #     self.drop_redundant_packs()

        
        # First pass at the packs: we split packs into:
        # * small packs
        # * big packs
        # * mixed/large packs (which NEED to be repacked)
        # * done packs, that are either big or small but already with
        #   the correct size
        #
        # IDEA for LARGE optimizations: use file names that reflect the
        # small/big distinction, and LEAVE them alone if they are
        # around the right size.
        done_pks = []
        redo_pks = []

        small_pks = []
        big_pks = []
        small_ics = []
        big_ics = []

        # Hmmm, this is where one needs to be smart ?
        for pk in self.list_packs():
            print "Reading pack %s " % pk.pack_file
            pk.read_pack()
            ics = sorted(pk.interconnected_objects.values(), key=lambda a: a.stats()['packed_size'])
            if len(ics) == 0:
                print " -> Empty pack ?"
                continue
            
            # Large packs, but made of only one object -> forget
            if pk.size > pack_size * tolerance:
                if len(ics) == 1:
                    done_pks.append(pk)
                    continue
            if pk.size > pack_size and pk.size < pack_size * tolerance:
                done_pks.append(pk)
                continue

            if (ics[0].stats()['packed_size'] < small_obj_size) and (ics[-1].stats()['packed_size'] > small_obj_size):
                for ic in ics:
                    if ic.stats()['packed_size'] < small_obj_size:
                        small_ics.append(ic)
                    else:
                        big_ics.append(ic)
                redo_pks.append(pk)
                continue

            # One size and large, we don't touch !
            if (ics[0].stats()['packed_size'] < small_obj_size):
                small_pks.append(pk)
                small_ics.extend(ics)
                redo_pks.append(pk)
            else:
                redo_pks.append(pk)
                big_pks.append(pk)
                big_ics.extend(ics)

        for d in done_pks:
            print "Not repacking %s, OK" % d.pack_file

        for d in redo_pks:
            print "Repacking  %s" % d.pack_file

        # Sort small things into packs
        small_pks = interconnected_objects.split_into_packs(small_ics, pack_size, tolerance)
        big_pks = interconnected_objects.split_into_packs(big_ics, pack_size, tolerance)
        if dry_run:
            print "Small packs:"
            i = 0
            for pk in small_pks:
                print "Pack #%d" % i
                i += 1
                for ic in pk:
                    print "   - %s " % ic.stats()
                    
            print "Large packs:"
            i = 0
            for pk in big_pks:
                print "Pack #%d" % i
                i += 1
                for ic in pk:
                    print "   - %s " % ic.stats()

        else:
            # First rename all packs to remove
            for pk in redo_pks:
                pk.rename("backup")
            
            for pk in small_pks:
                packfile.pack_objects(pk, "small")
            for pk in big_pks:
                packfile.pack_objects(pk, "big")

        for d in redo_pks:
            print "Removing pack: %s" % d.pack_file
            packfile.remove_pack(d)

                
            
            
        

class packed_object:
    def __init__(self, lst, pck):
        self.sha1 = lst[0]
        self.kind = lst[1]
        self.size = int(lst[2])
        self.packed_size = int(lst[3])
        self.file_size = None
        self.pack = pck
        self.base = None
        if len(lst) > 6:
            self.base_ref = lst[6]
        else:
            self.base_ref = False

    def root(self):
        """The root object"""
        if self.base is None:
            return self
        else:
            return self.base.root()

    def read_size(self):
        with os.popen('git cat-file -s %s' % self.sha1) as gcf:
            for line in gcf:
                self.file_size = int(line)
        return self.file_size


    def __str__(self):
        delta = ""
        if self.base:
            delta = " (delta)"
        
        return "object: %s, %s, size: %d/%d %s" % (self.sha1,
                                                   self.kind,
                                                   self.size,
                                                   self.packed_size,
                                                   delta)


class interconnected_objects:
    """A series of objects in a pack that are dependent on one another (in
terms of storage, i.e. objects that are deltas of a single root object"""

    def __init__(self, ele):
        self.root = ele.root()
        # Elements inside the 
        self.objects = {}
        self.add_object(ele)

    def is_sibling(self, obj):
        """Returns true if the object is a sibling (or already in) the pack"""
        return obj.root() == self.root

    def add_object(self, obj):
        if not self.is_sibling(obj):
            raise "Wrong object"

        while True:
            base = obj.base
            self.objects[obj] = base
            if base is None or base in self.objects:
                break
            obj = base

    def stats(self):
        st = {
            'objects': len(self.objects),
            'object_size': 0,
            'total_size': 0,
            'packed_size':0
        }
        for obj in self.objects.keys():
            st['object_size'] += obj.size
            if obj.file_size is not None:
                st['total_size'] += obj.file_size
            st['packed_size'] += obj.packed_size
        return st

    def sha1s(self):
        """Returns a list of all the SHA1 in the hierarchy"""
        return map(lambda o: o.sha1, self.objects.keys())
            
    @staticmethod
    def split_into_packs(ics, pack_size, tolerance):
        """Splits a list of ics into a list of list of ics,
        each that should make a pack of size between pack_size
        and pack_size*tolerance. This is probably going to fail.
        Returns a hash expected-size -> [objects] ?"""

        # A list of (expected_size, [ics])
        pks = [ ]
        for ic in ics:
            sz = ic.stats()['packed_size']
            fnd = False
            for tpk in pks:
                if tpk[0] > pack_size:
                    continue
                if tpk[0] + sz < pack_size * tolerance:
                    tpk[0] += sz
                    tpk[1].append(ic)
                    fnd = True
                    break
                    # todo: remove from the list ?
            if not fnd:
                pks.append([sz, [ic]])
        return map(lambda a: a[1], pks)
            
            

class packfile:
    def __init__(self, pck, rep):
        self.pack_file = pck
        self.size = os.path.getsize(pck)
        self.objects = {}
        self.interconnected_objects = {}
        self.rep = rep

    def __str__(self):
        return "packfile: %s, size: %d, %d objects" % (self.pack_file,
                                                       self.size,
                                                       len(self.objects))

    def dissect_path(self):
        """Returns [dirname, base, sha1]"""
        dn = os.path.dirname(self.pack_file)
        bn = os.path.basename(self.pack_file)
        b,c = bn.split("-")
        sha1 = c.split(".")[0]
        return [dn, b, sha1]

    def files(self):
        d, b, s = self.dissect_path()
        idx = os.path.join(d, "%s-%s.idx" % (b, s))
        return [self.pack_file, idx]

    @staticmethod
    def remove_pack(pk):
        fls = pk.files()
        for f in fls:
            os.unlink(f)

    def dump_objects(self):
        for o in self.objects.values():
            print " - %s %s" % (o.sha1, o.kind)
        print "Hierarchies"
        for i in self.interconnected_objects.values():
            print " - %s " % (" ".join(map(lambda o:o.sha1, i.objects.keys())))

    def rename(self, new_base):
        d, b, s = self.dissect_path()
        old_idx = os.path.join(d, "%s-%s.idx" % (b, s))
        new_pack = os.path.join(d, "%s-%s.pack" % (new_base, s))
        new_idx = os.path.join(d, "%s-%s.idx" % (new_base, s))
        os.rename(old_idx, new_idx)
        os.rename(self.pack_file, new_pack)
        self.pack_file = new_pack
    
    def read_pack(self):
        with os.popen('git verify-pack -v %s' % self.pack_file) as gvp:
            for line in gvp:
                lst = line.split()
                if len(lst[0]) == 40:
                    obj = packed_object(lst, self)
                    self.objects[obj.sha1] = obj
        # Now, we go over them again, cross-referencing them.
        for obj in self.objects.values():
            if obj.base_ref:
                obj.base = self.objects[obj.base_ref]
            else:
                obj.base = None

        # A pass to categorize the objects, has to come second for the
        # obj.base x-ref to be useable
        for obj in self.objects.values():
            root = obj.root()
            if root not in self.interconnected_objects:
                self.interconnected_objects[root] = interconnected_objects(obj)
            self.interconnected_objects[root].add_object(obj)
            

    def read_object_sizes(self):
        """Reads the sizes of all the objects"""
        proc = subprocess.Popen(['git','cat-file',
                                 '--batch-check=%(objectsize)'],
                                stdout=subprocess.PIPE,
                                stderr=subprocess.STDOUT,
                                stdin=subprocess.PIPE)
        objs = self.objects.values();
        inp = "\n".join(map(lambda x: x.sha1, objs))
        out, err = proc.communicate(inp)
        i = 0
        for ln in out.split("\n"):
            if len(ln) < 1:
                break
            objs[i].file_size = int(ln)
            i+= 1
        
    def stats(self):
        st = {
            'objects': 0,
            'object_size': 0,
            'total_size': 0,
            'deltas': 0,
            'interconnected_hierarchies': len(self.interconnected_objects),
            'packed_size':0
        }
        for obj in self.objects.values():
            st['objects'] += 1
            if obj.base is not None:
                st['deltas'] += 1
            st['object_size'] += obj.size
            if obj.file_size is not None:
                st['total_size'] += obj.file_size
            st['packed_size'] += obj.packed_size
        return st

    @staticmethod
    def repack(packs, dry_run=False):
        """Repacks the given packs"""
        if dry_run:
            print "Would repack packs: %s" % (", ".join(map(lambda x: x.pack_file, packs)))
                
        else:
            print "Repacking packs: %s" % (", ".join(map(lambda x: x.pack_file, packs)))
            with os.popen('git pack-objects .git/objects/pack/pack --threads=1', 'w') as gpa:
                for pck in packs:
                    for o in pck.objects.values():
                        gpa.write("%s\n" % o.sha1)


    @staticmethod
    def pack_objects(ics, base):
        """Packs the given object hierarchies into a pack with the given base name"""

        proc = subprocess.Popen(['git','pack-objects',
                                 '.git/objects/pack/%s' % base,
                                 "--threads=1"],
                                stdin=subprocess.PIPE)
        for ic in ics:
            for s in ic.sha1s():
                proc.stdin.write("%s\n" % s)
        proc.stdin.close()
        proc.wait()
        if proc.returncode != 0:
            raise "Failed to pack objects"


    def split(self, size, tolerance = 0.2):
        """Splits the pack into packs of approximately the given size"""
        ics = self.interconnected_objects.values()
        print len(ics)
        pcks = []               # List of arrays of ics
        for i in ics:
            fnd = False
            for p in pcks:
                sz = 0
                for ic in p:
                    sz += ic.stats()['packed_size']
                if (sz < size) and (sz + i.stats()['packed_size'] < (1 + tolerance) * size):
                    p.append(i)
                    fnd = True
                    continue
            if fnd:
                continue
            pcks.append([i])
        bs = "%s/split" % os.path.dirname(self.pack_file)
        cmd = 'git pack-objects %s --threads=1' % bs
        for p in pcks:
            sz = 0
            with os.popen(cmd, 'w') as gpa:
                for ic in p:
                    for o in ic.objects:
                        gpa.write("%s\n" % o.sha1)
            print p
            print " -> %d || %d " % (sz, size)
            

    @staticmethod
    def split_pack(fl, size, rep, tolerance = 0.2):
        pck = packfile(fl, rep)
        pck.read_pack()
        pck.split(size, tolerance)

            
            



                        
def mess_up(mu):
    """Creates a git repository in a subdirectory, and add files and modify them randomly"""
    orig_files = map(lambda x: "../%s" % x, glob.glob('*'))
    print orig_files
    if not os.path.isdir(".git/objects"):
        os.mkdir("tst-git")
    os.chdir("tst-git")
    if not os.path.isdir(".git"):
        os.system("git init")

    
    cur_files = []

    def call(*args):
        # print "Running: '%s'" % ("', '".join(args))
        subprocess.call(args)

    nbt = len(orig_files)/2
    if nbt < 1:
        nbt = 1
    elif nbt >= 100:
        nbt = 99
    
    while mu > 0:
        mu -= 1
        action = random.randint(0,1)
        nn = "file_%02d" % (random.randint(0, nbt))
        if action == 0:         # copy from below
            of = random.choice(orig_files)
            shutil.copy(of, nn)
            call('git', 'add', nn)
            call('git', 'commit', '-m', "Imported %s to %s" % (of, nn))
            cur_files.append(nn)
        elif len(cur_files) > 0:
            if action == 1:
                # Add some bits of the file
                of = random.choice(cur_files)
                if of == nn:
                    mu += 1
                    continue
                nb = random.randint(10,20)
                with open(of, "rb") as src:
                    with open(nn, 'wb') as dst:
                        bts = src.read()
                        dst.write(bts)
                        dst.write(os.urandom(nb))
                call('git', 'add', nn)
                call('git', 'commit', '-m', "Copied %s to %s and added %d bytes" % (of, nn, nb))
        else:
            mu += 1
            continue
        repack = random.randint(0,2) < 1
        if repack or mu == 0:
            print "\n\n\nRepack"
            call('git', 'repack')
            print "-> done\n"
            

        
            

if args.mess_up:
    mess_up(args.mess_up)
    exit()


rep = git_rep()

if args.dump_pack:
    pk = packfile(args.dump_pack, rep)
    pk.read_pack()
    pk.dump_objects()
    
    exit()

actions = {
    'repack': lambda: rep.simple_repack(args.size, dry_run = args.dry_run),
    'smart-repack': lambda: rep.smart_repack(args.big_objects, args.size, 1.2, args.dry_run),
    'split': lambda: packfile.split_pack(args.action[1], args.size, rep),
    'dump-objects': rep.dump_objects,
    'dump-full': lambda: rep.dump_objects(True)
}


for action in args.action:
    code = actions.get(action, None)
    if code is not None:
        code()
    else:
        print "Unkown action: %s" % action



#     # Now, walk over all the pack files, gathers the one that are smaller
# # than a predefined pack size until their whole size is about the threshold

# co = rep.count_objects()
# if co["size"] > args.repack_size:
#     rep.repack()

# rep.drop_redundant_packs()
    
# packs = []
# ts = 0
# for pack in rep.list_packs():
#     if pack.size < args.size*0.8:
#         pack.read_pack()
#         packs.append(pack)
#         ts += pack.size
#         if ts > 0.8*args.size:
#             packfile.repack(packs, args.dry_run)
#             packs = []
#             ts = 0
#         print pack
#     else:
#         print "Pack %s too big, not touching it" % pack

# rep.drop_redundant_packs()
